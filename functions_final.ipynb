{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import itertools\n",
    "import matplotlib.dates as mdates\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cleaning & aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanstations(list_of_csvs, stations_to_drop):\n",
    "    for i in np.arange(0,len(list_of_csvs)):\n",
    "        try:\n",
    "            list_of_csvs[i] = list_of_csvs[i].replace('(02102) Cardio Infantíl','(02102) Calle 161')\n",
    "            list_of_csvs[i] = list_of_csvs[i].replace('(03009) SHAIO','(03009) Av. Suba- Calle 116')\n",
    "            list_of_csvs[i] = list_of_csvs[i].replace('(05102) Mundo Aventura','(05102) Av. Américas - Av. Boyacá')\n",
    "            list_of_csvs[i] = list_of_csvs[i].replace('(06106) Corferias','(06106) Recinto Ferial')\n",
    "            list_of_csvs[i] = list_of_csvs[i].replace('(06108) Plaza de la Democracia','(06108) Concejo de Bogotá')\n",
    "            list_of_csvs[i] = list_of_csvs[i].replace('(07105) COLISEO','(07105) MOVISTAR ARENA')\n",
    "            list_of_csvs[i] = list_of_csvs[i].replace('(09115) Profamilia','(09115) Calle 34')\n",
    "            list_of_csvs[i] = list_of_csvs[i].replace('(10010) Hospitales','(10010) San Bernardo')\n",
    "        except:\n",
    "            continue\n",
    "    for j in np.arange(0,len(list_of_csvs)):\n",
    "        for k in np.arange(0,len(stations_to_drop)):\n",
    "            try:\n",
    "                list_of_csvs[j] = list_of_csvs[j][list_of_csvs[j]['Estación'] != stations_to_drop[k]]\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    return list_of_csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT USED\n",
    "# sortbystations_dailydata and aggregation_dailydata is for the day by day CSV files\n",
    "\n",
    "def sortbystations_dailydata(csv):\n",
    "    \n",
    "    '''function to sort all rides by station \n",
    "    for daily and weekly aggregation, we need to combine multiple days worth of CSVs beforehand \n",
    "    what's the column name for Stations ? it's not Acceso_Estacion'''\n",
    "    \n",
    "    stations = csv[\"Acceso_Estacion\"].unique() \n",
    "    grouped = csv.groupby([\"Acceso_Estacion\"]).count()\n",
    "    max_rides = max(grouped.iloc[:,1])\n",
    "    collection_df = pd.DataFrame()\n",
    "    for i in np.arange(0, len(stations)-1):\n",
    "        df_station = csv.loc[csv[\"Acceso_Estacion\"] == stations[i]]\n",
    "        times = df_station[\"Fecha_Transaccion\"].tolist()\n",
    "        title = stations[i]\n",
    "        if not len(times) == max_rides:\n",
    "            times.extend(['']*(max_rides-len(times)))\n",
    "        collection_df[title] = times\n",
    "    return collection_df\n",
    "\n",
    "def aggregation_dailydata(collection_df_onecolumn, rule):\n",
    "    '''\n",
    "    aggregation function\n",
    "    options for rule input: '15min' [by 15 min time periods], 'D' [by day], 'W' [by week], etc\n",
    "    options for collection_df_onecolumn input: select single column from collection_df (output of sortbystations function) to \n",
    "                                            see trips through one station OR input whole CSV to see trips through whole system'''\n",
    "    datetime = pd.to_datetime(collection_df_onecolumn)\n",
    "    frequency = pd.Series(np.ones(np.shape(collection_df_onecolumn)))\n",
    "    formatted_timefreq = pd.concat([datetime, frequency], axis=1)\n",
    "    formatted_timefreq.columns = ['DateTime', 'Frequency']\n",
    "    formatted_timefreq = formatted_timefreq.set_index('DateTime')\n",
    "    aggregated = formatted_timefreq['Frequency'].resample(rule).sum()\n",
    "    return aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this aggregation function is for the monthly summary CSV files\n",
    "# aggregates by station\n",
    "\n",
    "# in progress\n",
    "\n",
    "def aggregation_monthlydata(month_csv, days_in_month, stations):\n",
    "    fifteen_min = {}\n",
    "    daily = {}\n",
    "    weekly = {}\n",
    "    stations_in_data = month_csv[\"Estación\"].unique()\n",
    "    \n",
    "    for station in stations:\n",
    "        if station in stations_in_data:\n",
    "            station_grouped = month_csv[month_csv[\"Estación\"] == station].groupby('Intervalo').sum().iloc[:, 0:days_in_month]\n",
    "            rough_agg = station_grouped.transpose().stack().reset_index()\n",
    "            rough_agg = rough_agg.rename(columns={'level_0': 'date', 'Intervalo': '15 min interval', 0: 'frequency'})\n",
    "\n",
    "            # converting the dates from strings to datetime format\n",
    "            date_times = []\n",
    "            for i in np.arange(0,len(rough_agg)):\n",
    "                day = rough_agg['date'][i].strftime(\"%Y-%m-%d\")\n",
    "                date_string = day + rough_agg['15 min interval'][i]\n",
    "                date_times.append(datetime.strptime(date_string, '%Y-%m-%d%H:%M'))\n",
    "\n",
    "            rough_agg['datetime'] = date_times\n",
    "            aggregated_15min = rough_agg.set_index('datetime').drop(['date','15 min interval'],axis=1)\n",
    "            aggregated_day = aggregated_15min.resample('D').sum()\n",
    "            aggregated_week = aggregated_15min.resample('W').sum()\n",
    "\n",
    "            fifteen_min[station] = aggregated_15min\n",
    "            daily[station] = aggregated_day\n",
    "            weekly[station] = aggregated_week\n",
    "            \n",
    "    return [fifteen_min, daily, weekly]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_covid(stations, agg_type):\n",
    "    data_dict = {}\n",
    "    for i in np.arange(0,len(stations)):\n",
    "        s = stations[i]\n",
    "        training = pd.concat([jan2018_agg[agg_type][s], feb2018_agg[agg_type][s], march2018_agg[agg_type][s], \n",
    "                              april2018_agg[agg_type][s], may2018_agg[agg_type][s], june2018_agg[agg_type][s],\n",
    "                              july2018_agg[agg_type][s], aug2018_agg[agg_type][s], sept2018_agg[agg_type][s], \n",
    "                              oct2018_agg[agg_type][s], nov2018_agg[agg_type][s], dec2018_agg[agg_type][s],\n",
    "                              jan2019_agg[agg_type][s], feb2019_agg[agg_type][s], march2019_agg[agg_type][s], \n",
    "                              april2019_agg[agg_type][s], may2019_agg[agg_type][s], june2019_agg[agg_type][s],\n",
    "                              july2019_agg[agg_type][s], aug2019_agg[agg_type][s], sept2019_agg[agg_type][s], \n",
    "                              oct2019_agg[agg_type][s], nov2019_agg[agg_type][s], dec2019_agg[agg_type][s]])\n",
    "        test = pd.concat([jan2020_agg[agg_type][s], feb2020_agg[agg_type][s]])\n",
    "        covid = pd.concat([march2020_agg[agg_type][s], april2020_agg[agg_type][s], may2020_agg[agg_type][s]])\n",
    "        data_dict[s] = {'training': training,\n",
    "                        'test': test,\n",
    "                        'COVID': covid}\n",
    "        \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that displays regular plot, autocorrelation plot, and partial autocorrelation plot for each level of differencing\n",
    "# use autocorrelation plot to decide differencing (d) and AR (p)\n",
    "# use partial autocorrelation plot to decide MA (q)\n",
    "# note: AR factors correct for slight under-differencing, MA factors correct for slight over-differencing\n",
    "\n",
    "def differencing_acf_pacf_plots(aggregated):\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(20,15))\n",
    "    axes[0, 0].plot(aggregated); axes[0, 0].set_title('Original Series')\n",
    "    plot_acf(aggregated, ax=axes[0, 1])\n",
    "    plot_pacf(aggregated, ax=axes[0, 2])\n",
    "\n",
    "    # 1st Differencing\n",
    "    axes[1, 0].plot(aggregated.diff()); axes[1, 0].set_title('1st Order Differencing')\n",
    "    plot_acf(aggregated.diff().dropna(), ax=axes[1, 1])\n",
    "    plot_pacf(aggregated.diff().dropna(), ax=axes[1, 2])\n",
    "\n",
    "    # 2nd Differencing\n",
    "    axes[2, 0].plot(aggregated.diff().diff()); axes[2, 0].set_title('2nd Order Differencing')\n",
    "    plot_acf(aggregated.diff().diff().dropna(), ax=axes[2, 1])\n",
    "    plot_pacf(aggregated.diff().diff().dropna(), ax=axes[2, 2])\n",
    "\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_residual_plots(test_and_predictions_TESTdata, test_and_predictions_COVIDdata, figure_title=''):\n",
    "    fig, ax = plt.subplots(2, 2, sharey='row', figsize=(20, 15))\n",
    "    fig.suptitle(figure_title, fontsize=16)\n",
    "    \n",
    "    #Jan-Feb 2020\n",
    "    ax[0,0].plot(test_and_predictions_TESTdata['observed'], label='observed')\n",
    "    ax[0,0].plot(test_and_predictions_TESTdata['predictions'], label='predicted')\n",
    "    ax[0,0].set(xlabel=\"Date\",\n",
    "           ylabel=\"Ridership\",\n",
    "           title='July 2019-Feb 2020')\n",
    "    date_form = mdates.DateFormatter(\"%b-%d-%y\")\n",
    "    ax[0,0].xaxis.set_major_formatter(date_form)\n",
    "    ax[0,0].legend()\n",
    "    \n",
    "    errors = test_and_predictions_TESTdata['predictions'] - test_and_predictions_TESTdata['observed']\n",
    "    ax[1,0].scatter(test_and_predictions_TESTdata.index, pd.DataFrame(errors).set_index(test_and_predictions_TESTdata.index))\n",
    "    ax[1,0].plot(test_and_predictions_TESTdata.index, np.zeros(len(test_and_predictions_TESTdata.index)))\n",
    "    ax[1,0].set(xlabel=\"Date\",\n",
    "           ylabel=\"Residuals\",\n",
    "           title= 'Residual Distribution')\n",
    "    ax[1,0].xaxis.set_major_formatter(date_form)\n",
    "    \n",
    "    #March-July 2020\n",
    "    ax[0,1].plot(test_and_predictions_COVIDdata['observed'], label='observed')\n",
    "    ax[0,1].plot(test_and_predictions_COVIDdata['predictions'], label='predicted')\n",
    "    ax[0,1].set(xlabel=\"Date\",\n",
    "           ylabel=\"Ridership\",\n",
    "           title='March 2020-April 2021')\n",
    "    ax[0,1].xaxis.set_major_formatter(date_form)\n",
    "    ax[0,1].legend()\n",
    "    \n",
    "    errors = test_and_predictions_COVIDdata['predictions'] - test_and_predictions_COVIDdata['observed']\n",
    "    ax[1,1].scatter(test_and_predictions_COVIDdata.index, pd.DataFrame(errors).set_index(test_and_predictions_COVIDdata.index))\n",
    "    ax[1,1].plot(test_and_predictions_COVIDdata.index, np.zeros(len(test_and_predictions_COVIDdata.index)))\n",
    "    ax[1,1].set(xlabel=\"Date\",\n",
    "           ylabel=\"Residuals\",\n",
    "           title= 'Residual Distribution')\n",
    "    ax[1,1].xaxis.set_major_formatter(date_form)\n",
    "            \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rolling ARIMA/SARIMA functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to apply ARIMA to aggregated dataset, returns plot of predicted vs actual test dataset\n",
    "\n",
    "def applyARIMA_rolling(agg_training, agg_test, order=(2,0,0), plot=True, plot_title='', resid_plot=True):\n",
    "    train = agg_training.values\n",
    "    test = agg_test.values\n",
    "    history = [x for x in train]\n",
    "    predictions = list()\n",
    "    if agg_test.index[0] == datetime(2020,3,1):\n",
    "        for t in range(len(test) + 31 + 29):\n",
    "            model = ARIMA(history, order=order)\n",
    "            model_fit = model.fit()\n",
    "            output = model_fit.forecast()\n",
    "            yhat = output[0]\n",
    "            predictions.append(yhat)\n",
    "            if t < 60:\n",
    "                history.append(np.array([yhat]))\n",
    "            else:\n",
    "                obs = test[t-60]\n",
    "                history.append(obs)\n",
    "        test_and_predictions = pd.DataFrame({'observed': test.flatten(), 'predictions': predictions[60:]}).set_index(agg_test.index)\n",
    "    else:\n",
    "        for t in range(len(test)):\n",
    "            model = ARIMA(history, order=order)\n",
    "            model_fit = model.fit()\n",
    "            output = model_fit.forecast()\n",
    "            yhat = output[0]\n",
    "            predictions.append(yhat)\n",
    "            obs = test[t]\n",
    "            history.append(obs)\n",
    "        test_and_predictions = pd.DataFrame({'observed': test.flatten(), 'predictions': predictions}).set_index(agg_test.index)\n",
    "    \n",
    "    if plot==True:\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        ax.plot(test_and_predictions['observed'], label='observed')\n",
    "        ax.plot(test_and_predictions['predictions'], label='prediction')\n",
    "        ax.set(xlabel=\"Date\",\n",
    "               ylabel=\"Ridership\",\n",
    "               title=plot_title)\n",
    "        date_form = mdates.DateFormatter(\"%b-%d\")\n",
    "        ax.xaxis.set_major_formatter(date_form)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    if resid_plot==True:\n",
    "        errors = test_and_predictions['predictions'] - test_and_predictions['observed']\n",
    "        fig, ax = plt.subplots(figsize=(10,8))\n",
    "        ax.scatter(agg_test.index, pd.DataFrame(errors).set_index(agg_test.index))\n",
    "        ax.plot(agg_test.index, np.zeros(len(agg_test.index)))\n",
    "        date_form = mdates.DateFormatter(\"%b-%d\")\n",
    "        ax.xaxis.set_major_formatter(date_form)\n",
    "        plt.show()\n",
    "        \n",
    "    return {'observed and predicted values': test_and_predictions, 'model object': model_fit}\n",
    "\n",
    "# function to apply SARIMA to aggregated dataset, returns plot of predicted vs actual test dataset\n",
    "\n",
    "def applySARIMA_rolling(agg_training, agg_test, order=(2,0,0), seasonal_order=(0,0,0,0), plot=True, plot_title='', resid_plot=True):\n",
    "    train = agg_training.values\n",
    "    test = agg_test.values\n",
    "    history = [x for x in train]\n",
    "    predictions = list()\n",
    "    if agg_test.index[0] == datetime(2020,3,1):\n",
    "        for t in range(len(test) + 31 + 29):\n",
    "            #pdb.set_trace()\n",
    "            model = SARIMAX(history, order=order, seasonal_order=seasonal_order, initialization='approximate_diffuse')\n",
    "            model_fit = model.fit()\n",
    "            output = model_fit.forecast()\n",
    "            yhat = output[0]\n",
    "            predictions.append(yhat)\n",
    "            if t < 60:\n",
    "                history.append(np.array([yhat]))\n",
    "            else:\n",
    "                obs = test[t-60]\n",
    "                history.append(obs)\n",
    "        test_and_predictions = pd.DataFrame({'observed': test.flatten(), 'predictions': predictions[60:]}).set_index(agg_test.index)\n",
    "    else:\n",
    "        for t in range(len(test)):\n",
    "            model = SARIMAX(history, order=order, seasonal_order=seasonal_order)\n",
    "            model_fit = model.fit() \n",
    "            output = model_fit.forecast()\n",
    "            yhat = output[0]\n",
    "            predictions.append(yhat)\n",
    "            obs = test[t]\n",
    "            history.append(obs)\n",
    "        test_and_predictions = pd.DataFrame({'observed': test.flatten(), 'predictions': predictions}).set_index(agg_test.index)\n",
    "    \n",
    "    if plot==True:\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        ax.plot(test_and_predictions['observed'], label='observed')\n",
    "        ax.plot(test_and_predictions['predictions'], label='prediction')\n",
    "        ax.set(xlabel=\"Date\",\n",
    "               ylabel=\"Ridership\",\n",
    "               title=plot_title)\n",
    "        date_form = mdates.DateFormatter(\"%b-%d\")\n",
    "        ax.xaxis.set_major_formatter(date_form)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    if resid_plot==True:\n",
    "        errors = test_and_predictions['predictions'] - test_and_predictions['observed']\n",
    "        fig, ax = plt.subplots(figsize=(10,8))\n",
    "        ax.scatter(agg_test.index, pd.DataFrame(errors).set_index(agg_test.index))\n",
    "        ax.plot(agg_test.index, np.zeros(len(agg_test.index)))\n",
    "        date_form = mdates.DateFormatter(\"%b-%d\")\n",
    "        ax.xaxis.set_major_formatter(date_form)\n",
    "        plt.show()\n",
    "    \n",
    "    return {'observed and predicted values': test_and_predictions, 'model object': model_fit}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## static ARIMA/SARIMA functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyARIMA_static(agg_training, agg_test, order=(2,0,0), plot=True, plot_title='', resid_plot=True):\n",
    "    train = agg_training.values\n",
    "    test = agg_test.values\n",
    "    model = ARIMA(train, order=order)\n",
    "    model_fit = model.fit()\n",
    "    \n",
    "    if agg_test.index[0] == datetime(2020,3,1):\n",
    "        predictions = model_fit.forecast(steps=len(test) + 31 + 29)\n",
    "        predictions = predictions[60:]\n",
    "    else:\n",
    "        predictions = model_fit.forecast(steps=len(test))\n",
    "    test_and_predictions = pd.DataFrame({'observed': test.flatten(), 'predictions': predictions}).set_index(agg_test.index)\n",
    "    \n",
    "    if plot==True:\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        ax.plot(test_and_predictions['observed'], label='observed')\n",
    "        ax.plot(test_and_predictions['predictions'], label='prediction')\n",
    "        ax.set(xlabel=\"Date\",\n",
    "               ylabel=\"Ridership\",\n",
    "               title=plot_title)\n",
    "        date_form = mdates.DateFormatter(\"%b-%d\")\n",
    "        ax.xaxis.set_major_formatter(date_form)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    if resid_plot==True:\n",
    "        errors = test_and_predictions['predictions'] - test_and_predictions['observed']\n",
    "        fig, ax = plt.subplots(figsize=(10,8))\n",
    "        ax.scatter(agg_test.index, pd.DataFrame(errors).set_index(agg_test.index))\n",
    "        ax.plot(agg_test.index, np.zeros(len(agg_test.index)))\n",
    "        date_form = mdates.DateFormatter(\"%b-%d\")\n",
    "        ax.xaxis.set_major_formatter(date_form)\n",
    "        plt.show()\n",
    "        \n",
    "    return {'observed and predicted values': test_and_predictions, 'model object': model_fit}\n",
    "\n",
    "def applySARIMA_static(agg_training, agg_test, order=(2,0,0), seasonal_order=(0,0,0,0), plot=True, plot_title='', resid_plot=True):\n",
    "    train = agg_training.values\n",
    "    test = agg_test.values\n",
    "    model = SARIMAX(train, order=order, seasonal_order=seasonal_order)\n",
    "    model_fit = model.fit()\n",
    "    \n",
    "    if agg_test.index[0] == datetime(2020,3,1):\n",
    "        predictions = model_fit.forecast(steps=len(test) + 31 + 29)\n",
    "        predictions = predictions[60:]\n",
    "    else:\n",
    "        predictions = model_fit.forecast(steps=len(test))\n",
    "    test_and_predictions = pd.DataFrame({'observed': test.flatten(), 'predictions': predictions}).set_index(agg_test.index)\n",
    "    \n",
    "    if plot==True:\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        ax.plot(test_and_predictions['observed'], label='observed')\n",
    "        ax.plot(test_and_predictions['predictions'], label='prediction')\n",
    "        ax.set(xlabel=\"Date\",\n",
    "               ylabel=\"Ridership\",\n",
    "               title=plot_title)\n",
    "        date_form = mdates.DateFormatter(\"%b-%d\")\n",
    "        ax.xaxis.set_major_formatter(date_form)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    if resid_plot==True:\n",
    "        errors = test_and_predictions['predictions'] - test_and_predictions['observed']\n",
    "        fig, ax = plt.subplots(figsize=(10,8))\n",
    "        ax.scatter(agg_test.index, pd.DataFrame(errors).set_index(agg_test.index))\n",
    "        ax.plot(agg_test.index, np.zeros(len(agg_test.index)))\n",
    "        date_form = mdates.DateFormatter(\"%b-%d\")\n",
    "        ax.xaxis.set_major_formatter(date_form)\n",
    "        plt.show()\n",
    "    return {'observed and predicted values': test_and_predictions, 'model object': model_fit}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## intermediate ARIMA/SARIMA functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applySARIMA_intermediate(agg_training, agg_test, order=(2,0,0), seasonal_order=(0,0,0,0), agg_type='daily', plot=True, plot_title='', resid_plot=True):\n",
    "    train = agg_training\n",
    "    test = agg_test\n",
    "    predictions = []\n",
    "    \n",
    "    if agg_type=='15 min':\n",
    "        pred_window = 4\n",
    "    elif agg_type=='daily':\n",
    "        pred_window = 7\n",
    "    elif agg_type=='weekly':\n",
    "        pred_window = 4\n",
    "    \n",
    "    model = SARIMAX(train, order=order, seasonal_order=seasonal_order)\n",
    "    model_fit = model.fit()\n",
    "    for x in model_fit.forecast(pred_window).tolist():\n",
    "        predictions.append(x)\n",
    "    \n",
    "    k=0\n",
    "    while pred_window*k+(pred_window-1) < len(test.values):\n",
    "        week_startdate = test.index[pred_window*k].strftime('%Y-%m-%d')\n",
    "        week_enddate = test.index[pred_window*k+(pred_window-1)].strftime('%Y-%m-%d')\n",
    "        model_fit = model_fit.append(test[week_startdate:week_enddate])\n",
    "        for x in model_fit.forecast(pred_window).tolist():\n",
    "            predictions.append(x)\n",
    "        k=k+1\n",
    "    \n",
    "    test_and_predictions = pd.DataFrame({'observed': test.values.flatten(), 'predictions': predictions[:len(test.values)]}).set_index(test.index)\n",
    "    \n",
    "    if plot==True:\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        ax.plot(test_and_predictions['observed'], label='observed')\n",
    "        ax.plot(test_and_predictions['predictions'], label='prediction')\n",
    "        ax.set(xlabel=\"Date\",\n",
    "               ylabel=\"Ridership\",\n",
    "               title=plot_title)\n",
    "        date_form = mdates.DateFormatter(\"%b-%d\")\n",
    "        ax.xaxis.set_major_formatter(date_form)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    if resid_plot==True:\n",
    "        errors = test_and_predictions['predictions'] - test_and_predictions['observed']\n",
    "        fig, ax = plt.subplots(figsize=(10,8))\n",
    "        ax.scatter(agg_test.index, pd.DataFrame(errors).set_index(agg_test.index))\n",
    "        ax.plot(agg_test.index, np.zeros(len(agg_test.index)))\n",
    "        date_form = mdates.DateFormatter(\"%b-%d\")\n",
    "        ax.xaxis.set_major_formatter(date_form)\n",
    "        plt.show()\n",
    "        \n",
    "    return {'observed and predicted values': test_and_predictions, 'model object': model_fit}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluating models / gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models_ARIMA(dataset, p_values, d_values, q_values):\n",
    "    dataset = dataset.astype('float32')\n",
    "    best_score, best_order = float(\"0\"), None\n",
    "    for p in p_values:\n",
    "        for d in d_values:\n",
    "            for q in q_values:\n",
    "                order = (p,d,q)\n",
    "                try:\n",
    "                    test_and_predictions, model_fit = applyARIMA(dataset,order=order,plot=False)\n",
    "                    corr = test_and_predictions.corr().iloc[1,0]\n",
    "                    if corr > best_score:\n",
    "                        best_score, best_order = corr, order\n",
    "                    #print('ARIMA%s R^2=%.3f' % (order,corr))\n",
    "                except:\n",
    "                    continue\n",
    "    print('Best ARIMA%s R^2=%.3f' % (best_order, best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_sample_from_parameters(hyperparam_dict, size = 10):\n",
    "    ''' Returns a DataFrame with len size of random draws from the grid. \n",
    "    Input:\n",
    "    hyperparam_dict: dictionary with keys p/d/q and P/D/Q/m, if applicable \n",
    "    size = Number of random draws\n",
    "    '''\n",
    "    if len(hyperparam_dict)==3:\n",
    "        p = hyperparam_dict['p']\n",
    "        d = hyperparam_dict['d']\n",
    "        q = hyperparam_dict['q']\n",
    "        all_param_combinations = itertools.product(p,d,q)\n",
    "        column_labels = ['p','d','q']\n",
    "    if len(hyperparam_dict)==7:\n",
    "        p = hyperparam_dict['p']; d = hyperparam_dict['d']; q = hyperparam_dict['q']\n",
    "        P = hyperparam_dict['P']\n",
    "        D = hyperparam_dict['D']\n",
    "        Q = hyperparam_dict['Q']\n",
    "        m = hyperparam_dict['m']\n",
    "        all_param_combinations = list(itertools.product(p,d,q,P,D,Q,m))\n",
    "        column_labels = ['p','d','q','P','D','Q','m']\n",
    "        \n",
    "    grid = pd.DataFrame(all_param_combinations, columns=column_labels)\n",
    "    grid_rands = grid.iloc[np.random.choice(grid.index, size, replace=False),:] \n",
    "    return grid_rands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(hyperparam_dict, agg_training, agg_test, model='ARIMA', model_type='intermediate', agg_type='daily', iterations = 50):\n",
    "    \n",
    "    random_grid = create_random_sample_from_parameters(hyperparam_dict, size = iterations)\n",
    "    score_list = []\n",
    "    normBIC_list = []\n",
    "    MAPE_list = []\n",
    "    \n",
    "    for x in np.arange(0,len(random_grid)):\n",
    "        #print(x)\n",
    "        if model=='ARIMA':\n",
    "            order = random_grid.iloc[x,:]\n",
    "            try:\n",
    "                if model_type=='rolling':\n",
    "                    output = applyARIMA_rolling(agg_training,agg_test,order=order,plot=False,resid_plot=False)\n",
    "                elif model_type=='static':\n",
    "                    output = applyARIMA_static(agg_training,agg_test,order=order,plot=False,resid_plot=False)\n",
    "                \n",
    "                test_and_predictions = output['observed and predicted values']\n",
    "                \n",
    "                corr = test_and_predictions.corr().iloc[1,0] ** 2\n",
    "                score_list.append(corr)\n",
    "                MSE = mean_squared_error(test_and_predictions['observed'],test_and_predictions['predictions'])\n",
    "                normBIC = np.log(MSE) + sum(order+1)*np.log(output['model object'].nobs)/output['model object'].nobs\n",
    "                normBIC_list.append(normBIC)\n",
    "                MAPE_list.append(MAPE(test_and_predictions))\n",
    "                if normBIC == np.nanmin(normBIC_list):\n",
    "                    final_hp = order\n",
    "                    final_output = output\n",
    "            except:\n",
    "                score_list.append(np.nan)\n",
    "                normBIC_list.append(np.nan)\n",
    "                MAPE_list.append(np.nan)\n",
    "                continue\n",
    "        elif model=='SARIMA':\n",
    "            order = random_grid.iloc[x,0:3]\n",
    "            s_order = random_grid.iloc[x,3:8]\n",
    "            try:\n",
    "                if model_type=='rolling':\n",
    "                    output = applySARIMA_rolling(agg_training,agg_test,order=order,seasonal_order=s_order,plot=False,resid_plot=False)\n",
    "                elif model_type=='static':\n",
    "                    output = applySARIMA_static(agg_training,agg_test,order=order,seasonal_order=s_order,plot=False,resid_plot=False)\n",
    "                elif model_type=='intermediate':\n",
    "                    output = applySARIMA_intermediate(agg_training, agg_test, order=order, seasonal_order=s_order, agg_type=agg_type, plot=False, resid_plot=False)\n",
    "                \n",
    "                test_and_predictions = output['observed and predicted values']\n",
    "                \n",
    "                #pdb.set_trace()\n",
    "                try:\n",
    "                    MSE = mean_squared_error(test_and_predictions['observed'],test_and_predictions['predictions'])\n",
    "                    normBIC = np.log(MSE) + sum(order+1)*np.log(output['model object'].nobs)/output['model object'].nobs\n",
    "                    normBIC_list.append(normBIC)\n",
    "                    MAPE_list.append(MAPE(test_and_predictions))\n",
    "                    corr = test_and_predictions.corr().iloc[1,0] ** 2\n",
    "                    score_list.append(corr)\n",
    "                    if normBIC == np.nanmin(normBIC_list):\n",
    "                        final_hp = [order, s_order]\n",
    "                        final_output = output\n",
    "                except ValueError: \n",
    "                    normBIC_list.append(np.nan)\n",
    "                    MAPE_list.append(np.nan)\n",
    "                    score_list.append(np.nan)\n",
    "                    continue\n",
    "            except:\n",
    "                normBIC_list.append(np.nan)\n",
    "                MAPE_list.append(np.nan)\n",
    "                score_list.append(np.nan)\n",
    "                continue\n",
    "    \n",
    "    results = random_grid\n",
    "    results['R-squared'] = score_list\n",
    "    results['normalized BIC'] = normBIC_list\n",
    "    results['MAPE'] = MAPE_list\n",
    "    \n",
    "    return {'final hyperparams': final_hp, \n",
    "            'observed and predicted values': final_output['observed and predicted values'],\n",
    "            'model object': final_output['model object'],\n",
    "            'gridsearch results': results}\n",
    "    #return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAPE function\n",
    "# filters out time periods with 0 actual trips and replaces negative predicted trips with 0\n",
    "\n",
    "def MAPE(test_and_predictions):\n",
    "    predictions = test_and_predictions['predictions']\n",
    "    for i in np.arange(0,len(predictions)-1):\n",
    "        if predictions[i] < 0:\n",
    "            predictions[i] = 0\n",
    "    data = pd.DataFrame({'observed': test_and_predictions['observed'],\n",
    "                        'predictions': predictions})\n",
    "    data = data[data['observed'] != 0]\n",
    "    return np.mean(np.abs((data['observed'] - data['predictions']) / data['observed'])) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VAPE(test_and_predictions):\n",
    "    predictions = test_and_predictions['predictions']\n",
    "    for i in np.arange(0,len(predictions)-1):\n",
    "        if predictions[i] < 0:\n",
    "            predictions[i] = 0\n",
    "    errors = test_and_predictions['observed'] - predictions\n",
    "    return np.mean((errors - np.mean(errors))**2) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(test_and_predictions):\n",
    "    predictions = test_and_predictions['predictions']\n",
    "    for i in np.arange(0,len(predictions)-1):\n",
    "        if predictions[i] < 0:\n",
    "            predictions[i] = 0\n",
    "    data = test_and_predictions\n",
    "    data = data[data['observed'] != 0]\n",
    "    return np.sqrt(np.mean((data['observed'] - data['predictions'])**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## automating process - one station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollingSARIMA_results(cleaned_data_onestation, hyperparam_dict, agg_type='daily', log_transform=True):\n",
    "    fifteen_min = cleaned_data_onestation\n",
    "    daily = cleaned_data_onestation.resample('D').sum()\n",
    "    weekly = cleaned_data_onestation.resample('W').sum()\n",
    "    \n",
    "    if agg_type=='15 min':\n",
    "        train = fifteen_min.loc['2018-01-01':'2019-12-31'] # August 2015 - June 2019\n",
    "        test = fifteen_min.loc['2020-01-01':'2020-02-29'] # July 2019 - Feb 2020\n",
    "        covid = fifteen_min.loc['2020-03-01':'2020-05-31'] # March 2020 - April 2021\n",
    "    elif agg_type=='daily':\n",
    "        train = daily.loc['2018-01-01':'2019-12-31']\n",
    "        test = daily.loc['2020-01-01':'2020-02-29']\n",
    "        covid = daily.loc['2020-03-01':'2020-05-31']\n",
    "    elif agg_type=='weekly':\n",
    "        train = weekly.loc['2018-01-01':'2019-12-31']\n",
    "        test = weekly.loc['2020-01-01':'2020-02-29']\n",
    "        covid = weekly.loc['2020-03-01':'2020-05-31']\n",
    "    else:\n",
    "        print('options for agg_type: 15 min, daily, weekly')\n",
    "        return\n",
    "    \n",
    "    if log_transform==True:\n",
    "        train = np.log(train)\n",
    "        test = np.log(test)\n",
    "        covid = np.log(covid)\n",
    "    \n",
    "    results_test = grid_search(hyperparam_dict, train, test, model='SARIMA', model_type='rolling', iterations = 10)\n",
    "    best_order = results_test['final hyperparams'][0].values\n",
    "    best_s_order = results_test['final hyperparams'][1].values\n",
    "    \n",
    "    results_covid = applySARIMA_rolling(train, covid, order=best_order, seasonal_order=best_s_order, plot=False, resid_plot=False)\n",
    "    \n",
    "    fig_title = cleaned_data_onestation.columns[0] + '\\nSARIMA' + str(best_order) + str(best_s_order)\n",
    "    prediction_residual_plots(results_test['observed and predicted values'], results_covid['observed and predicted values'], figure_title=fig_title)\n",
    "    \n",
    "    return {'final hyperparams': results_test['final hyperparams'],\n",
    "           'predicted values (test)': results_test['observed and predicted values'],\n",
    "           'model object (test)': results_test['model object'],\n",
    "           'gridsearch results': results_test['gridsearch results'],\n",
    "           'predicted values (COVID)': results_covid['observed and predicted values'],\n",
    "           'model object (COVID)': results_covid['model object']}\n",
    "    \n",
    "    #return [results_test, results_covid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def staticSARIMA_results(cleaned_data_onestation, hyperparam_dict, agg_type='daily', log_transform=True):\n",
    "    fifteen_min = cleaned_data_onestation\n",
    "    daily = cleaned_data_onestation.resample('D').sum()\n",
    "    weekly = cleaned_data_onestation.resample('W').sum()\n",
    "    \n",
    "    if agg_type=='15 min':\n",
    "        train = fifteen_min.loc['2015-08-01':'2019-06-30'] # August 2015 - June 2019\n",
    "        test = fifteen_min.loc['2019-07-01':'2020-02-29'] # July 2019 - Feb 2020\n",
    "        covid = fifteen_min.loc['2020-03-01':'2021-04-30'] # March 2020 - April 2021\n",
    "    elif agg_type=='daily':\n",
    "        train = daily.loc['2015-08-01':'2019-06-30']\n",
    "        test = daily.loc['2019-07-01':'2020-02-29']\n",
    "        covid = daily.loc['2020-03-01':'2021-04-30']\n",
    "    elif agg_type=='weekly':\n",
    "        train = weekly.loc['2015-08-01':'2019-06-30']\n",
    "        test = weekly.loc['2019-07-01':'2020-02-29']\n",
    "        covid = weekly.loc['2020-03-01':'2021-04-30']\n",
    "    else:\n",
    "        print('options for agg_type: 15 min, daily, weekly')\n",
    "        return\n",
    "    \n",
    "    if log_transform==True:\n",
    "        train = np.log(train+1)\n",
    "        test = np.log(test+1)\n",
    "        covid = np.log(covid+1)\n",
    "    \n",
    "    results_test = grid_search(hyperparam_dict, train, test, model='SARIMA', model_type='static', iterations = 10)\n",
    "    best_order = results_test['final hyperparams'][0].values\n",
    "    best_s_order = results_test['final hyperparams'][1].values\n",
    "    \n",
    "    results_covid = applySARIMA_static(train, covid, order=best_order, seasonal_order=best_s_order, plot=False, resid_plot=False)\n",
    "    \n",
    "    fig_title = cleaned_data_onestation.columns[0] + '\\nSARIMA' + str(best_order) + str(best_s_order)\n",
    "    prediction_residual_plots(np.exp(results_test['observed and predicted values']), np.exp(results_covid['observed and predicted values']), figure_title=fig_title)\n",
    "    \n",
    "    return {'final hyperparams': results_test['final hyperparams'],\n",
    "           'predicted values (test)': results_test['observed and predicted values'],\n",
    "           'model object (test)': results_test['model object'],\n",
    "           'gridsearch results': results_test['gridsearch results'],\n",
    "           'predicted values (COVID)': results_covid['observed and predicted values'],\n",
    "           'model object (COVID)': results_covid['model object']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intermediateSARIMA_results(cleaned_data_onestation, hyperparam_dict, agg_type='daily', log_transform=True):\n",
    "    fifteen_min = cleaned_data_onestation\n",
    "    daily = cleaned_data_onestation.resample('D').sum()\n",
    "    weekly = cleaned_data_onestation.resample('W').sum()\n",
    "    \n",
    "    if agg_type=='15 min':\n",
    "        train = fifteen_min.loc['2015-08-01':'2019-06-30'] # August 2015 - June 2019\n",
    "        test = fifteen_min.loc['2019-07-01':'2020-02-29'] # July 2019 - Feb 2020\n",
    "        covid = fifteen_min.loc['2020-03-01':'2021-04-30'] # March 2020 - April 2021\n",
    "    elif agg_type=='daily':\n",
    "        train = daily.loc['2015-08-01':'2019-06-30']\n",
    "        test = daily.loc['2019-07-01':'2020-02-29']\n",
    "        covid = daily.loc['2020-03-01':'2021-04-30']\n",
    "    elif agg_type=='weekly':\n",
    "        train = weekly.loc['2015-08-01':'2019-06-30']\n",
    "        test = weekly.loc['2019-07-01':'2020-02-29']\n",
    "        covid = weekly.loc['2020-03-01':'2021-04-30']\n",
    "    else:\n",
    "        print('options for agg_type: 15 min, daily, weekly')\n",
    "        return\n",
    "    \n",
    "    if log_transform==True:\n",
    "        train = np.log(train+1)\n",
    "        test = np.log(test+1)\n",
    "        covid = np.log(covid+1)\n",
    "    \n",
    "    results_test = grid_search(hyperparam_dict, train, test, model='SARIMA', model_type='intermediate', agg_type='weekly',iterations = 10)\n",
    "    best_order = results_test['final hyperparams'][0].values\n",
    "    best_s_order = results_test['final hyperparams'][1].values\n",
    "    \n",
    "    results_covid = applySARIMA_intermediate(train, pd.concat([test,covid]), order=best_order, seasonal_order=best_s_order, agg_type=agg_type, plot=False, resid_plot=False)\n",
    "    \n",
    "    fig_title = cleaned_data_onestation.columns[0] + '\\nSARIMA' + str(best_order) + str(best_s_order)\n",
    "    prediction_residual_plots(np.exp(results_test['observed and predicted values']), np.exp(results_covid['observed and predicted values']['2020-03-01':'2021-04-30']), figure_title=fig_title)\n",
    "    \n",
    "    return {'final hyperparams': results_test['final hyperparams'],\n",
    "           'predicted values (test)': results_test['observed and predicted values'],\n",
    "           'model object (test)': results_test['model object'],\n",
    "           'gridsearch results': results_test['gridsearch results'],\n",
    "           'predicted values (COVID)': results_covid['observed and predicted values'],\n",
    "           'model object (COVID)': results_covid['model object']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
